{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pymysql\n",
    "HOST, ADMIN, PASS, DB =  'mivo-db.cj2ucwgierrs.us-east-1.rds.amazonaws.com', 'admin', '58#64KDashz^bLrqTG2', 'mivo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dtype_spec = {\n",
    "    'Tipo': 'int',\n",
    "    'Numero': 'int',\n",
    "    'Lin': 'int',\n",
    "    'C.Bod': 'int',\n",
    "    'Cant': 'int',\n",
    "    'Venta': 'int',\n",
    "    'Costo': 'int',\n",
    "    'Margen': 'int',\n",
    "    'Jerarquia': str,\n",
    "    'Sub Jerarquia': str,\n",
    "    'Local': str,\n",
    "    'Color': str,\n",
    "    'talla': str,\n",
    "    'Marca': str,\n",
    "    'temporada': str,\n",
    "    'Rut': str,\n",
    "    'Cliente': str\n",
    "}\n",
    "df = pd.read_csv('Ventas.csv', usecols=list(dtype_spec.keys()) + ['Fecha'], dtype=dtype_spec)\n",
    "df['Fecha'] = pd.to_datetime(df['Fecha'], format='%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Local'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flujo = pd.read_csv('Flujo.csv')\n",
    "flujo['FECHA'] = pd.to_datetime(flujo['FECHA'], format='%m/%d/%Y')\n",
    "flujo['Total Visitas'] = flujo.iloc[:, 2:26].sum(axis=1)\n",
    "flujoSimple = flujo[['FECHA','ENTRADA', 'Total Visitas']]\n",
    "flujoSimple.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data types for the hourly columns from 00 to 23\n",
    "hour_columns_dtypes = {f\"{hour:02}\": int for hour in range(24)}\n",
    "\n",
    "# 'ENTRADA' is specified as a string\n",
    "dtype_dict = {\n",
    "    'ENTRADA': str,\n",
    "    **hour_columns_dtypes  # Merge the dictionary with hourly data types\n",
    "}\n",
    "\n",
    "# Load the CSV file with defined data types and specify 'FECHA' as a date column\n",
    "traffic_df = pd.read_csv('Flujo.csv', dtype=dtype_dict, parse_dates=['FECHA'])\n",
    "\n",
    "# Display the first few entries to check data loading\n",
    "print(traffic_df.head())\n",
    "\n",
    "# Check the data types to confirm proper loading and type assignment\n",
    "#print(traffic_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'df' is already loaded and contains your sales data\n",
    "\n",
    "# Ensure 'Fecha' is a datetime type for proper grouping\n",
    "df['Fecha'] = pd.to_datetime(df['Fecha'])\n",
    "\n",
    "# Group by 'Fecha' and 'Local', and count distinct 'Numero'\n",
    "daily_sales = df.groupby(['Fecha', 'Local']).agg(purchases=('Numero', 'nunique')).reset_index()\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(daily_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visits = pd.DataFrame(flujoSimple)\n",
    "sales = pd.DataFrame(daily_sales)\n",
    "\n",
    "# Normalize the date columns\n",
    "visits['FECHA'] = pd.to_datetime(visits['FECHA'])\n",
    "sales['Fecha'] = pd.to_datetime(sales['Fecha'])\n",
    "\n",
    "\n",
    "# Normalize the local/entrada names to lowercase\n",
    "visits['ENTRADA'] = visits['ENTRADA'].astype(str).str.lower().str.strip()\n",
    "sales['Local'] = sales['Local'].astype(str).str.lower().str.strip()\n",
    "\n",
    "# Rename columns for consistency\n",
    "visits.rename(columns={'FECHA': 'date', 'ENTRADA': 'local'}, inplace=True)\n",
    "sales.rename(columns={'Fecha': 'date', 'Local': 'local'}, inplace=True)\n",
    "\n",
    "# Merge DataFrames\n",
    "result = pd.merge(visits, sales, on=['date', 'local'], how='inner')\n",
    "result['conversion_rate'] = result['purchases'] / result['Total Visitas'] * 100\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dtype_spec = {\n",
    "    'Tipo': 'int',\n",
    "    'Numero': 'int',\n",
    "    'Lin': 'int',\n",
    "    'C.Bod': 'int',\n",
    "    'Cant': 'int',\n",
    "    'Venta': 'int',\n",
    "    'Costo': 'int',\n",
    "    'Margen': 'int',\n",
    "    'Jerarquia': str,\n",
    "    'Sub Jerarquia': str,\n",
    "    'Local': str,\n",
    "    'Color': str,\n",
    "    'talla': str,\n",
    "    'Marca': str,\n",
    "    'temporada': str,\n",
    "    'Rut': str,\n",
    "    'Cliente': str\n",
    "}\n",
    "ventas = pd.read_csv('Ventas.csv', usecols=list(dtype_spec.keys()) + ['Fecha'], dtype=dtype_spec)\n",
    "ventas['Fecha'] = pd.to_datetime(ventas['Fecha'], format='%Y%m%d')\n",
    "daily_sales = ventas.groupby(['Fecha', 'Local']).agg(purchases=('Numero', 'nunique')).reset_index()\n",
    "# clients_sales = ventas.groupby(['Cliente']).agg(\n",
    "#     Total_Venta=('Venta', 'sum'),\n",
    "#     Item_Count=('Numero', 'nunique'),\n",
    "#     ).reset_index()\n",
    "\n",
    "\n",
    "flujo = pd.read_csv('Flujo.csv')\n",
    "flujo['FECHA'] = pd.to_datetime(flujo['FECHA'], format='%m/%d/%Y')\n",
    "flujo['Total Visitas'] = flujo.iloc[:, 2:26].sum(axis=1)\n",
    "flujoSimple = flujo[['FECHA','ENTRADA', 'Total Visitas']]\n",
    "\n",
    "visits = pd.DataFrame(flujoSimple)\n",
    "sales = pd.DataFrame(daily_sales)\n",
    "\n",
    "# Normalize the date columns\n",
    "visits['FECHA'] = pd.to_datetime(visits['FECHA'])\n",
    "sales['Fecha'] = pd.to_datetime(sales['Fecha'])\n",
    "\n",
    "\n",
    "# Normalize the local/entrada names to lowercase\n",
    "visits['ENTRADA'] = visits['ENTRADA'].astype(str).str.lower().str.strip()\n",
    "sales['Local'] = sales['Local'].astype(str).str.lower().str.strip()\n",
    "\n",
    "# Rename columns for consistency\n",
    "visits.rename(columns={'FECHA': 'date', 'ENTRADA': 'local'}, inplace=True)\n",
    "sales.rename(columns={'Fecha': 'date', 'Local': 'local'}, inplace=True)\n",
    "\n",
    "# Merge DataFrames\n",
    "result = pd.merge(visits, sales, on=['date', 'local'], how='inner')\n",
    "result['conversion_rate'] = result['purchases'] / result['Total Visitas'] * 100\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert sales to DB [WORKING]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pymysql\n",
    "\n",
    "dtype_spec = {\n",
    "    'Tipo': 'int',\n",
    "    'Numero': 'int',\n",
    "    'Lin': 'int',\n",
    "    'C.Bod': 'int',\n",
    "    'Cant': 'int',\n",
    "    'Venta': 'int',\n",
    "    'Costo': 'int',\n",
    "    'Margen': 'int',\n",
    "    'Jerarquia': str,\n",
    "    'Sub Jerarquia': str,\n",
    "    'Local': str,\n",
    "    'Color': str,\n",
    "    'talla': str,\n",
    "    'Marca': str,\n",
    "    'temporada': str,\n",
    "    'Rut': str,\n",
    "    'Cliente': str\n",
    "}\n",
    "ventas = pd.read_csv('Ventas.csv', usecols=list(dtype_spec.keys()) + ['Fecha'], dtype=dtype_spec)\n",
    "ventas['Fecha'] = pd.to_datetime(ventas['Fecha'], format='%Y%m%d')\n",
    "daily_sales = ventas.groupby(['Fecha', 'Local']).agg(purchases=('Numero', 'nunique')).reset_index()\n",
    "\n",
    "\n",
    "sales = pd.DataFrame(daily_sales)\n",
    "\n",
    "\n",
    "## Only some stores\n",
    "salesFiltered = pd.DataFrame(sales)\n",
    "\n",
    "# Mapping dictionary\n",
    "local_map = {\n",
    "    'concepcion': 1,\n",
    "    'santos dumontt': 2,\n",
    "    'mall tobalaba pte.alto': 3,\n",
    "    'mall quilin': 4\n",
    "}\n",
    "\n",
    "# Apply the map to the 'local' column\n",
    "salesFiltered['store_id'] = salesFiltered['local'].map(local_map).astype('Int64')\n",
    "\n",
    "# Filter out rows where 'mapped_value' is None\n",
    "filtered_sales = salesFiltered.dropna(subset=['store_id'])\n",
    "\n",
    "# Print the filtered DataFrame\n",
    "filtered_sales.head()\n",
    "\n",
    "\n",
    "\n",
    "conn = pymysql.connect(host=HOST, user=ADMIN, password=PASS, database=DB)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO sales (purchases, date, store_id)\n",
    "VALUES (%s, %s, %s);\n",
    "\"\"\"\n",
    "\n",
    "# Iterate over DataFrame rows\n",
    "for index, row in filtered_sales.iterrows():\n",
    "    # Here you might add checks or transformations if needed\n",
    "    # Prepare data for insertion\n",
    "    data_to_insert = (row['purchases'], row['date'], row['store_id'])\n",
    "    \n",
    "    # Execute the SQL command\n",
    "    try:\n",
    "        cursor.execute(insert_query, data_to_insert)\n",
    "        conn.commit()  # Commit changes for the current row only\n",
    "    except pymysql.Error as e:\n",
    "        print(f\"Error on row {index}: {e}\")\n",
    "        conn.rollback()  # Roll back the current row insertion only\n",
    "        continue  # Continue with the next row\n",
    "\n",
    "# Close the connection\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert visits-flujo to DB [Working]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diego/miniconda3/lib/python3.11/site-packages/openpyxl/styles/stylesheet.py:226: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n",
      "/home/diego/miniconda3/lib/python3.11/site-packages/openpyxl/styles/stylesheet.py:226: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n",
      "/home/diego/miniconda3/lib/python3.11/site-packages/openpyxl/styles/stylesheet.py:226: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n",
      "/home/diego/miniconda3/lib/python3.11/site-packages/openpyxl/styles/stylesheet.py:226: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'store_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3791\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3790\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3792\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'store_id'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m total_visits\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m     36\u001b[0m     date \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFECHA\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mdate()  \u001b[38;5;66;03m# Ensuring date is in proper format\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m     store_id \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstore_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hour \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m24\u001b[39m):\n\u001b[1;32m     39\u001b[0m         hour_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhour\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Format hour as two digits\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pandas/core/series.py:1040\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1040\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_value(key)\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pandas/core/series.py:1156\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1156\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mget_loc(label)\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3798\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3794\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3795\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3796\u001b[0m     ):\n\u001b[1;32m   3797\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3798\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3799\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3800\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3801\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3803\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'store_id'"
     ]
    }
   ],
   "source": [
    "tobalaba = pd.read_csv('tobalaba_flujo.csv')\n",
    "conce = pd.read_csv('conce_flujo.csv')\n",
    "quilin = pd.read_csv('quilin_flujo.csv')\n",
    "\n",
    "df_panoramico = pd.read_excel('/home/diego/Downloads/vivo_panomarico.xls', skiprows=4)\n",
    "df_plaza_los_rios = pd.read_excel('/home/diego/Downloads/plaza_los_rios.xls', skiprows=4)\n",
    "df_estado = pd.read_excel('/home/diego/Downloads/estado.xls', skiprows=4)\n",
    "df_apumanque = pd.read_excel('/home/diego/Downloads/apumanque.xls', skiprows=4)\n",
    "\n",
    "df_panoramico['FECHA'] = pd.to_datetime(df_panoramico['FECHA'], format='%Y-%m-%d')\n",
    "df_plaza_los_rios['FECHA'] = pd.to_datetime(df_plaza_los_rios['FECHA'], format='%Y-%m-%d')\n",
    "df_estado['FECHA'] = pd.to_datetime(df_estado['FECHA'], format='%Y-%m-%d')\n",
    "df_apumanque['FECHA'] = pd.to_datetime(df_apumanque['FECHA'], format='%Y-%m-%d')\n",
    "\n",
    "df_panoramico['store_id'] = 11\n",
    "df_plaza_los_rios['store_id'] = 9\n",
    "df_estado['store_id'] = 12\n",
    "df_apumanque['store_id'] = 10\n",
    "\n",
    "\n",
    "total_visits = pd.concat([df_panoramico, df_plaza_los_rios, df_estado, df_apumanque], ignore_index=True)\n",
    "\n",
    "# Connect to the database\n",
    "conn = pymysql.connect(host=HOST, user=ADMIN, password=PASS, database=DB)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# SQL Insert Statement\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO visits (count, time, date, store_id)\n",
    "VALUES (%s, %s, %s, %s);\n",
    "\"\"\"\n",
    "\n",
    "# Prepare data for insertion\n",
    "data_to_insert = []\n",
    "for index, row in total_visits.iterrows():\n",
    "    date = pd.to_datetime(row['FECHA']).date()  # Ensuring date is in proper format\n",
    "    store_id = row['store_id']\n",
    "    for hour in range(24):\n",
    "        hour_key = f'{hour:02}'  # Format hour as two digits\n",
    "        count = row[hour_key]\n",
    "        if count > 0:\n",
    "            time = f'{hour_key}:00:00'  # Format time as HH:MM:SS\n",
    "            data_to_insert.append((count, time, date, store_id))\n",
    "\n",
    "# Insert data in chunks of 1000\n",
    "chunk_size = 1000\n",
    "for i in range(0, len(data_to_insert), chunk_size):\n",
    "    chunk = data_to_insert[i:i + chunk_size]\n",
    "    try:\n",
    "        cursor.executemany(insert_query, chunk)\n",
    "        conn.commit()\n",
    "    except pymysql.Error as e:\n",
    "        print(f\"Error inserting data: {e}\")\n",
    "        conn.rollback()  # Roll back the transaction\n",
    "\n",
    "# Close the connection\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sales per category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dtype_spec = {\n",
    "    'Tipo': 'int',\n",
    "    'Numero': 'int',\n",
    "    'Lin': 'int',\n",
    "    'C.Bod': 'int',\n",
    "    'Cant': 'int',\n",
    "    'Venta': 'int',\n",
    "    'Costo': 'int',\n",
    "    'Margen': 'int',\n",
    "    'Jerarquia': str,\n",
    "    'Sub Jerarquia': str,\n",
    "    'Local': str,\n",
    "    'Color': str,\n",
    "    'talla': str,\n",
    "    'Marca': str,\n",
    "    'temporada': str,\n",
    "    'Rut': str,\n",
    "    'Cliente': str\n",
    "}\n",
    "# Assuming 'df' is your DataFrame already loaded with the data\n",
    "sales_df = pd.read_csv('Ventas.csv', usecols=list(dtype_spec.keys()) + ['Fecha'], dtype=dtype_spec)\n",
    "sales_df['Fecha'] = pd.to_datetime(sales_df['Fecha'], format='%Y%m%d')\n",
    "# Define conditions\n",
    "conditions = [\n",
    "    sales_df['Local'].str.strip().isin(['INTERNET']),  # Internet category\n",
    "    sales_df['Local'].str.strip().isin(['BODEGA DOMINICA','BODEGA LDP', 'MAYORISTA', 'DARDIGNAC', 'BUENOS AIRES', 'SANTOS DUMONTT']),  # Mayoristas category\n",
    "    sales_df['Local'].str.strip().isin(['CONCEPCION', 'VALDIVIA', 'MALL TOBALABA PTE.ALTO', 'APUMANQUE', 'PROVIDENCIA', 'ESTADO', 'MALL QUILIN'])  # No mayoristas category\n",
    "]\n",
    "\n",
    "# Define choices corresponding to conditions\n",
    "choices = [\n",
    "    'Internet',  # Choice for Internet\n",
    "    'Mayoristas',  # Choice for Mayoristas\n",
    "    'No mayoristas'  # Choice for No mayoristas\n",
    "]\n",
    "\n",
    "# Create new column 'categories' based on conditions\n",
    "sales_df['categories'] = np.select(conditions, choices, default='No mayoristas')  # Default can be adjusted if needed\n",
    "\n",
    "# Display the DataFrame with new categories column\n",
    "sales_df[['Local', 'categories']].value_counts()\n",
    "\n",
    "\n",
    "category_sales = sales_df.groupby('categories').agg(\n",
    "    total_sales=('Venta', 'sum'),                # Sum of the 'Venta' column for total sales\n",
    "    total_margin=('Margen', 'sum'),             # Sum of the 'Margen' column for total margins\n",
    "    count_items=('Numero', 'count'),             # Count of all occurrences of 'Numero'\n",
    "    unique_sales=('Numero', 'nunique')         # Count of unique 'Numero' values\n",
    ").reset_index()\n",
    "\n",
    "\n",
    "\n",
    "sales_df['Fecha_Year_Month'] = sales_df['Fecha'].dt.strftime('%Y-%m')\n",
    "\n",
    "category_sales = sales_df.groupby(['Local', 'Fecha_Year_Month']).agg(\n",
    "    total_sales=('Venta', 'sum'),                # Sum of the 'Venta' column for total sales\n",
    "    total_margin=('Margen', 'sum'),             # Sum of the 'Margen' column for total margins\n",
    "    count_items=('Numero', 'count'),             # Count of all occurrences of 'Numero'\n",
    "    unique_sales=('Numero', 'nunique'),        # Count of unique 'Numero' values\n",
    "    category=('categories', 'first')           # Get the first category value\n",
    ").reset_index()\n",
    "# category_sales_sorted = category_sales.sort_values(by='total_sales', ascending=False)\n",
    "# print(category_sales_sorted)\n",
    "\n",
    "# sales_df['Local'].value_counts()\n",
    "category_sales.head(n=100)\n",
    "# category_sales.to_json('category_sales.json', orient='records', lines=True)\n",
    "\n",
    "# sales_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert into Sales_agg [Working]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'Local' and 'Fecha_Year_Month', aggregating various metrics\n",
    "\n",
    "\n",
    "category_sales = sales_df.groupby(['Local', 'Fecha_Year_Month']).agg(\n",
    "    total_sales=('Venta', 'sum'),                # Sum of the 'Venta' column for total sales\n",
    "    total_margin=('Margen', 'sum'),             # Sum of the 'Margen' column for total margins\n",
    "    count_items=('Numero', 'count'),            # Count of all occurrences of 'Numero'\n",
    "    unique_sales=('Numero', 'nunique'),         # Count of unique 'Numero' values\n",
    "    category=('categories', 'first')            # Get the first category value\n",
    ").reset_index()\n",
    "\n",
    "# Calculate 'Average Items per Transaction' by dividing the count of items by the count of unique sales\n",
    "category_sales['Average Items per Transaction'] = category_sales['count_items'] / category_sales['unique_sales']\n",
    "\n",
    "# Optionally, round the 'Average Items per Transaction' to a sensible number of decimal places, e.g., 2\n",
    "category_sales['Average Items per Transaction'] = category_sales['Average Items per Transaction'].round(2)\n",
    "\n",
    "# Sort the resulting DataFrame by 'total_sales' if needed\n",
    "category_sales_sorted = category_sales.sort_values(by='total_sales', ascending=False)\n",
    "\n",
    "# Display the top 100 entries in the DataFrame\n",
    "category_sales_sorted.head(n=1000)\n",
    "\n",
    "\n",
    "store_dict = {\n",
    "\t'CONCEPCION': 1,\n",
    "\t'SANTOS DUMONTT': 2,\n",
    "\t'MALL TOBALABA PTE.ALTO': 3,\n",
    "\t'MALL QUILIN': 4,\n",
    "\t'INTERNET': 5,\n",
    "\t'DARDIGNAC': 6,\n",
    "\t'BUENOS AIRES': 7,\n",
    "\t'BODEGA LDP': 8,\n",
    "\t'VALDIVIA': 9,\n",
    "\t'APUMANQUE': 10,\n",
    "\t'PROVIDENCIA': 11,\n",
    "\t'ESTADO': 12,\n",
    "\t'MAYORISTA': 13,\n",
    "\t'BODEGA DOMINICA': 14\n",
    "}\n",
    "\n",
    "# INTERNET                          73734\n",
    "# SANTOS DUMONTT                    57076\n",
    "# DARDIGNAC                         54580\n",
    "# BUENOS AIRES                      35024\n",
    "# BODEGA LDP                        26897\n",
    "# CONCEPCION                        21063\n",
    "# VALDIVIA                          14119\n",
    "# MALL TOBALABA PTE.ALTO            12252\n",
    "# APUMANQUE                         10110\n",
    "# PROVIDENCIA                        8777\n",
    "# ESTADO                             6994\n",
    "# MALL QUILIN                        1990\n",
    "# MAYORISTA                           486\n",
    "# BODEGA DOMINICA                     449\n",
    "\n",
    "# '1', 'concepcion', 'Concepción', '1', '3', NULL, NULL\n",
    "# '2', 'santos-dumont', 'Santos Dumont', '1', '2', NULL, NULL\n",
    "# '3', 'tobalaba', 'Tobalaba', '1', '3', NULL, NULL\n",
    "# '4', 'quilin', 'Quilin', '1', '3', NULL, NULL\n",
    "# '5', 'internet', 'Internet', '1', '1', NULL, NULL\n",
    "# '6', 'dardignac', 'Dardignac', '1', '2', NULL, NULL\n",
    "# '7', 'buenos_aires', 'Buenos Aires', '1', '2', NULL, NULL\n",
    "# '8', 'bodega_ldp', 'Bodega LDP', '1', '2', NULL, NULL\n",
    "# '9', 'valdivia', 'Valdivia', '1', '3', NULL, NULL\n",
    "# '10', 'apumanque', 'Apumanque', '1', '3', NULL, NULL\n",
    "# '11', 'providencia', 'Providencia', '1', '3', NULL, NULL\n",
    "# '12', 'estado', 'Estado', '1', '3', NULL, NULL\n",
    "# '13', 'mayorista', 'Mayoristas', '1', '2', NULL, NULL\n",
    "# '14', 'bodega_dominica', 'Bodega Dominica', '1', '2', NULL, NULL\n",
    "\n",
    "\n",
    "\n",
    "conn = pymysql.connect(host=HOST, user=ADMIN, password=PASS, database=DB)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# SQL Insert Statement\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO sales_agg (date, store_id, total_sales, total_margin, count_items, unique_sales) VALUES (%s, %s, %s, %s, %s, %s);\n",
    "\"\"\"\n",
    "\n",
    "# Iterate over DataFrame rows\n",
    "for index, row in category_sales_sorted.iterrows():\n",
    "    # Prepare data for insertion\n",
    "\tstore_id = store_dict.get(row['Local'].upper().strip(), None)\n",
    "\tif store_id is None:\n",
    "\t\tprint(f\"Store ID not found for '{row['Local']}'\")\n",
    "\t\tcontinue\n",
    "\n",
    "\tdata_to_insert = (f\"{row['Fecha_Year_Month']}-01\", store_id, row['total_sales'], row['total_margin'], row['count_items'], row['unique_sales'])\n",
    "\t\n",
    "\t# Execute the SQL command\n",
    "\ttry:\n",
    "\t\tcursor.execute(insert_query, data_to_insert)\n",
    "\t\tconn.commit()  # Commit changes for the current row only\n",
    "\texcept pymysql.Error as e:\n",
    "\t\tprint(f\"Error on row {index}: {e}\")\n",
    "\t\tconn.rollback()  # Roll back the current row insertion only\n",
    "\t\tcontinue  # Continue with the next row\n",
    "\n",
    "\n",
    "# Close the connection\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df['Local'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
