{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group by ID (only to view not final utility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logs/conce_bbox.csv_alternative.csv\n",
      "       ID  Total Number\n",
      "85     14           191\n",
      "192    15           118\n",
      "500    16            53\n",
      "212    33           112\n",
      "514    78            49\n",
      "..    ...           ...\n",
      "446  5671            63\n",
      "167  5692           128\n",
      "582  5693            22\n",
      "143  5697           138\n",
      "572  5704            29\n",
      "\n",
      "[593 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "BASE_FOLDER_NAME = 'logs'\n",
    "CSV_FILE_PATH = 'conce_bbox.csv_alternative.csv'\n",
    "\n",
    "\n",
    "filename = os.path.join(BASE_FOLDER_NAME, CSV_FILE_PATH)\n",
    "print(filename)\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "id_counts = df['id'].value_counts().reset_index()\n",
    "id_counts.columns = ['ID', 'Total Number']\n",
    "\n",
    "print(id_counts.sort_values(by='ID', ascending=True))\n",
    "# Save the result to a new CSV file\n",
    "output_file_path = os.path.join(BASE_FOLDER_NAME, 'id_counts.csv')\n",
    "\n",
    "id_counts.to_csv(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data transformer CSV (Yolo) -> Feature Engineering (CSV) [Irrelevant]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12258/4013622069.py:8: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(filename)\n"
     ]
    }
   ],
   "source": [
    "# This now is innessecary because was already performed by the detection script\n",
    "\n",
    "# import pandas as pd\n",
    "# import os\n",
    "\n",
    "# # Assuming previous setup\n",
    "# BASE_FOLDER_NAME = 'logs'\n",
    "# CSV_FILE_PATH = 'conce_bbox.csv_alternative.csv'\n",
    "# filename = os.path.join(BASE_FOLDER_NAME, CSV_FILE_PATH)\n",
    "# df = pd.read_csv(filename)\n",
    "\n",
    "# # Calculate Area, centroid_x, and centroid_y\n",
    "# df['Area'] = (df['x2'] - df['x1']) * (df['y2'] - df['y1'])\n",
    "# df['centroid_x'] = ((df['x1'] + df['x2']) / 2).astype(int)\n",
    "# df['centroid_y'] = ((df['y1'] + df['y2']) / 2).astype(int)\n",
    "\n",
    "# # Selecting relevant columns and create a copy to avoid SettingWithCopyWarning\n",
    "# new_df = df[['id', 'Area', 'centroid_x', 'centroid_y', 'frame_number', 'overlap', 'distance_to_center', 'score']].copy()\n",
    "\n",
    "# # Renaming columns\n",
    "# new_df.columns = ['id', 'area', 'centroid_x', 'centroid_y', 'frame_number', 'overlap', 'distance_to_center', 'conf_score']\n",
    "\n",
    "# # Convert distance_to_center to int and round overlap and conf_score using .loc to avoid warnings\n",
    "# new_df.loc[:, 'distance_to_center'] = new_df['distance_to_center'].astype(int)\n",
    "# new_df.loc[:, 'overlap'] = new_df['overlap'].round(2)\n",
    "# new_df.loc[:, 'conf_score'] = new_df['conf_score'].round(2)\n",
    "\n",
    "# # Define new file name and save the modified DataFrame\n",
    "# new_file = CSV_FILE_PATH.split('.')[0] + '_area.csv'\n",
    "# new_file = os.path.join(BASE_FOLDER_NAME, new_file)\n",
    "# new_df.to_csv(new_file, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create CSV -> JSON with Random Image Distribute Linealrly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "# Example usage\n",
    "BASE_FOLDER_NAME = 'logs'\n",
    "base_folder = '/home/diego/Documents/yolov7-tracker/imgs_conce'\n",
    "k_folds = 4\n",
    "n_images = 5\n",
    "default_rate = '-'\n",
    "JSON_FILE_NAME = 'selected_images.json'\n",
    "\n",
    "def select_images_from_folds(base_folder, k_folds, n_images):\n",
    "    results = []\n",
    "    \n",
    "    id_folders = [f.path for f in os.scandir(base_folder) if f.is_dir()]\n",
    "    \n",
    "    for folder in id_folders:\n",
    "        images = sorted(glob.glob(f\"{folder}/*.png\"), key=lambda x: int(x.split('_')[-2]))\n",
    "        \n",
    "        # If there are not enough images to select from, select all images in the folder\n",
    "        if len(images) < n_images * k_folds:\n",
    "            for img_path in images:\n",
    "                img_id = os.path.basename(folder)  # Assuming the folder name is the ID\n",
    "                results.append({\n",
    "                    \"img\": img_path,\n",
    "                    \"id\": int(img_id),\n",
    "                    \"rate\": default_rate\n",
    "                })\n",
    "            continue  # Skip to the next folder\n",
    "        \n",
    "        kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "        \n",
    "        for _, test_index in kf.split(images):\n",
    "            selected_indices = np.random.choice(test_index, min(n_images, len(test_index)), replace=False)\n",
    "            selected_images = np.array(images)[selected_indices]\n",
    "            \n",
    "            for img_path in selected_images:\n",
    "                img_id = os.path.basename(folder)  # Assuming the folder name is the ID\n",
    "                results.append({\n",
    "                    \"img\": img_path,\n",
    "                    \"id\": int(img_id),\n",
    "                    \"rate\": default_rate\n",
    "                })\n",
    "\n",
    "    # Sort the results by 'id'\n",
    "    sorted_results = sorted(results, key=lambda x: x['id'])\n",
    "    \n",
    "    return sorted_results\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    selected_images = select_images_from_folds(base_folder, k_folds, n_images)\n",
    "    filename = os.path.join(BASE_FOLDER_NAME, JSON_FILE_NAME)\n",
    "    # To save the result to a JSON file in sorted order by ID\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(selected_images, f, indent=2)\n",
    "except Exception as e:  # Generic exception handling, if needed\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### READ JSON LABEL and Complete CSV (Lee el rate del JSON y lo pasa al CSV, para el modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Setup file paths\n",
    "BASE_FOLDER_NAME = 'logs'\n",
    "JSON_FILE = 'selected_images_MODEL.json'\n",
    "JSON_FILE_PATH = os.path.join(BASE_FOLDER_NAME, JSON_FILE)\n",
    "CSV_FILE_PATH = 'conce_bbox_area.csv'\n",
    "CSV_FILE_PATH = os.path.join(BASE_FOLDER_NAME, CSV_FILE_PATH)\n",
    "\n",
    "# Load the CSV data into a DataFrame\n",
    "df = pd.read_csv(CSV_FILE_PATH)\n",
    "\n",
    "# Load the JSON data\n",
    "with open(JSON_FILE_PATH, 'r') as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "# Initialize a new column 'label' with default values (e.g., '-')\n",
    "df['label'] = '-'\n",
    "\n",
    "# Iterate over each object in the JSON data\n",
    "for item in json_data:\n",
    "    # Extract the frame_number from the image path\n",
    "    # Assuming the format is always like the provided examples, \n",
    "    # and the frame_number is the part just before the last underscore\n",
    "    frame_number = int(item['img'].split('_')[3])\n",
    "    # Get the ID directly from the item\n",
    "    id_ = item['id']\n",
    "    # Get the rate\n",
    "    rate = item['rate']\n",
    "    \n",
    "    # Find the row that matches both ID and frame_number, then update the 'label' column\n",
    "    df.loc[(df['id'] == id_) & (df['frame_number'] == frame_number), 'label'] = rate\n",
    "\n",
    "# Optional: Save the updated DataFrame to a new CSV file\n",
    "updated_csv_file_path = os.path.join(BASE_FOLDER_NAME, 'updated_' + CSV_FILE_PATH.split('/')[-1])\n",
    "df.to_csv(updated_csv_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9722222222222222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8094/1637288565.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  predict_df['model_label'] = predicted_labels\n",
      "/tmp/ipykernel_8094/1637288565.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  predict_df['model_confidence'] = predicted_confidences\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated CSV saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "BASE_FOLDER_NAME = 'logs'\n",
    "CSV_FILE_PATH = 'updated_conce_bbox_area.csv'\n",
    "CSV_FILE_PATH = os.path.join(BASE_FOLDER_NAME, CSV_FILE_PATH)\n",
    "\n",
    "NEW_CSV = os.path.join(BASE_FOLDER_NAME, 'updated_conce_bbox_area_model.csv')\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv(CSV_FILE_PATH)\n",
    "\n",
    "# Preprocess your data\n",
    "# Convert '-' labels to None for easier handling\n",
    "df['label'] = df['label'].apply(lambda x: None if x == '-' else x).astype(float)\n",
    "\n",
    "# Separate the dataset into training and prediction sets\n",
    "train_df = df.dropna(subset=['label'])\n",
    "predict_df = df[df['label'].isna()]\n",
    "\n",
    "# Define features and target\n",
    "features = ['area', 'centroid_x', 'centroid_y', 'frame_number', 'overlap', 'distance_to_center', 'conf_score']\n",
    "target = 'label'\n",
    "\n",
    "# Splitting the training data for validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_df[features], train_df[target], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Gradient Boosting Classifier\n",
    "model = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Validate the model\n",
    "val_predictions = model.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val, val_predictions)\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "# Predicting on the dataset without labels\n",
    "predict_features = predict_df[features]\n",
    "predicted_labels = model.predict(predict_features)\n",
    "predicted_confidences = model.predict_proba(predict_features).max(axis=1)\n",
    "\n",
    "# Adding predictions back to the dataframe\n",
    "predict_df['model_label'] = predicted_labels\n",
    "predict_df['model_confidence'] = predicted_confidences\n",
    "\n",
    "# Combine the prediction and training dataframes\n",
    "final_df = pd.concat([train_df, predict_df], sort=False)\n",
    "\n",
    "# Save the updated dataframe to a new CSV file\n",
    "final_df.to_csv(NEW_CSV, index=False)\n",
    "\n",
    "print(\"Updated CSV saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST Show model results predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Read the CSV file\n",
    "BASE_FOLDER_NAME = 'logs'\n",
    "CSV_FILE_PATH = 'updated_conce_bbox_area.csv'\n",
    "NEW_CSV = os.path.join(BASE_FOLDER_NAME, 'updated_conce_bbox_area_model.csv')\n",
    "\n",
    "def find_matching_file_path(directory, filename_start):\n",
    "    \"\"\"\n",
    "    Searches for files in the specified directory that start with the given filename start string.\n",
    "    \n",
    "    :param directory: The directory to search within.\n",
    "    :param filename_start: The initial part of the file name to match.\n",
    "    :return: The full path to the first matching file, or None if no match is found.\n",
    "    \"\"\"\n",
    "    # Construct the search pattern\n",
    "    search_pattern = os.path.join(directory, filename_start + \"*.png\")\n",
    "    \n",
    "    # Use glob to find all files matching the pattern\n",
    "    matching_files = glob.glob(search_pattern)\n",
    "    \n",
    "    # Return the first matching file path, if any\n",
    "    if matching_files:\n",
    "        return matching_files[0]  # Return full path of the first match\n",
    "    else:\n",
    "        return ''  # No match found\n",
    "\n",
    "df = pd.read_csv(NEW_CSV)\n",
    "\n",
    "# Filter rows where `model_label` and `model_confidence` are not null\n",
    "df_filtered = df.dropna(subset=['model_label', 'model_confidence'])\n",
    "\n",
    "# Filtering rows where frame_number % 3 == 0\n",
    "#df_filtered = df_filtered[df_filtered['frame_number'] % 3 == 0]\n",
    "\n",
    "# Base path for the images\n",
    "base_path = \"/home/diego/Documents/yolov7-tracker/imgs_conce\"\n",
    "\n",
    "# Function to construct the file path\n",
    "def construct_file_path(row):\n",
    "    return os.path.join(base_path, str(int(row['id'])))\n",
    "\n",
    "# Apply the function to construct file paths\n",
    "df_filtered['file_path'] = df_filtered.apply(construct_file_path, axis=1)\n",
    "\n",
    "# Randomly select 50 images if available, or take the whole dataset if less than 50\n",
    "sample_size = min(20, len(df_filtered))\n",
    "sampled_df = df_filtered.sample(n=sample_size)\n",
    "\n",
    "rows = (sample_size + 4) // 5  # Calculate rows needed for the sample size, adjust the denominator to change columns\n",
    "cols = 5 if sample_size > 5 else sample_size  # Adjust columns based on sample size\n",
    "\n",
    "fig, axs = plt.subplots(rows, cols, figsize=(20, 8 * rows))  # Adjust figsize dynamically\n",
    "axs = axs.flatten()  # Flatten to easily loop over if it's a grid\n",
    "\n",
    "for i in range(len(axs)):\n",
    "    if i < sample_size:\n",
    "        row = sampled_df.iloc[i]\n",
    "        img_path = find_matching_file_path(row.file_path, f\"img_{row.id}_{row.frame_number}\")\n",
    "        if os.path.exists(img_path):\n",
    "            img = Image.open(img_path)\n",
    "            axs[i].imshow(img)\n",
    "            axs[i].set_title(f\"ID: {row.id}\\nFrame: {row.frame_number}\\nLabel: {int(row.model_label)}\\nConfidence: {row.model_confidence:.2f}\", fontsize=10)\n",
    "            axs[i].axis('off')\n",
    "        else:\n",
    "            axs[i].set_visible(False)\n",
    "    else:\n",
    "        axs[i].set_visible(False)  # Hide unused subplots\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data process IN/OUT/BAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated CSV saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "BASE_FOLDER_NAME = 'logs'\n",
    "CSV_FILE_PATH = 'conce_bbox.csv_alternative.csv'\n",
    "CSV_FILE_PATH = os.path.join(BASE_FOLDER_NAME, CSV_FILE_PATH)\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv(CSV_FILE_PATH)\n",
    "\n",
    "# Ensure the DataFrame is sorted by 'id' and 'frame_number' for correct diff calculations\n",
    "df.sort_values(by=['id', 'frame_number'], inplace=True)\n",
    "\n",
    "# Calculate Movement Features (Δx and Δy)\n",
    "df['delta_x'] = df.groupby('id')['centroid_x'].diff().fillna(0)\n",
    "df['delta_y'] = df.groupby('id')['centroid_y'].diff().fillna(0)\n",
    "\n",
    "# Calculate Aggregated Features for each ID\n",
    "aggregations = {\n",
    "    'delta_x': ['mean', 'max', 'min', 'std'],\n",
    "    'delta_y': ['mean', 'max', 'min', 'std']\n",
    "}\n",
    "aggregated_features = df.groupby('id').agg(aggregations).reset_index()\n",
    "\n",
    "# Correct the naming of the aggregated columns\n",
    "aggregated_features.columns = ['id'] + [f'{var}_{stat}' for var, stats in aggregations.items() for stat in stats]\n",
    "\n",
    "# Correctly merge aggregated features back to the original dataframe\n",
    "df = pd.merge(df, aggregated_features, on='id', how='left')\n",
    "\n",
    "# Calculate Sequence Features (net movement direction)\n",
    "df['net_movement_x'] = df.groupby('id')['delta_x'].transform('sum')\n",
    "df['net_movement_y'] = df.groupby('id')['delta_y'].transform('sum')\n",
    "\n",
    "df.to_csv(CSV_FILE_PATH, index=False)\n",
    "\n",
    "print(\"Updated CSV saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In vs Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12258/3192550527.py:15: DtypeWarning: Columns (21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(CSV_FILE_PATH)\n",
      "/tmp/ipykernel_12258/3192550527.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['label_direction_encoded'] = label_encoder.fit_transform(df_filtered['label_direction'])  # 'IN' -> 1, 'OUT' -> 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 1.0\n",
      "Updated CSV with predictions saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "BASE_FOLDER_NAME = 'logs'\n",
    "CSV_FILE_PATH = 'conce_bbox.csv_alternative.csv'\n",
    "NEW_FILE = 'updated_conce_bbox_with_predictions.csv'\n",
    "CSV_FILE_PATH = os.path.join(BASE_FOLDER_NAME, CSV_FILE_PATH)\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(CSV_FILE_PATH)\n",
    "\n",
    "# Filter out rows with 'BAD' or empty in 'label_direction'\n",
    "df_filtered = df[(df['label_direction'] == 'IN') | (df['label_direction'] == 'OUT')]\n",
    "\n",
    "# Correctly encode 'IN' as 1 and 'OUT' as 0\n",
    "label_encoder = LabelEncoder()\n",
    "df_filtered['label_direction_encoded'] = label_encoder.fit_transform(df_filtered['label_direction'])  # 'IN' -> 1, 'OUT' -> 0\n",
    "\n",
    "# Define features (make sure to only include numeric columns and exclude any text columns)\n",
    "features = [col for col in df_filtered.columns if col not in ['id', 'label_direction', 'label_direction_encoded'] and df_filtered[col].dtype in [np.int64, np.float64]]\n",
    "target = 'label_direction_encoded'\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_filtered[features], df_filtered[target], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Gradient Boosting Classifier\n",
    "model = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "predictions = model.predict(X_test)\n",
    "prediction_probs = model.predict_proba(X_test)[:, 1]  # Probability of being 'IN'\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "# Prepare the entire dataset for prediction\n",
    "# First, ensure only numeric features are used\n",
    "df['label_direction_p'] = np.nan  # Initialize column for model predictions\n",
    "df['label_direction_p_conf'] = np.nan  # Initialize column for prediction confidence\n",
    "\n",
    "# Predicting on rows needing prediction (assuming 'BAD' or not labeled)\n",
    "predict_features = df[features]\n",
    "df['label_direction_p'] = model.predict(predict_features)\n",
    "df['label_direction_p_conf'] = model.predict_proba(predict_features)[:, 1]  # Probability of being 'IN'\n",
    "\n",
    "# Mapping numeric predictions back to 'IN' or 'OUT'\n",
    "df['label_direction_p'] = label_encoder.inverse_transform(df['label_direction_p'].astype(int))\n",
    "\n",
    "# Save the updated dataframe to a new CSV file\n",
    "NEW_CSV_PATH = os.path.join(BASE_FOLDER_NAME, NEW_FILE)\n",
    "df.to_csv(NEW_CSV_PATH, index=False)\n",
    "\n",
    "print(\"Updated CSV with predictions saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In|OUT vs BAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12258/3214391915.py:13: DtypeWarning: Columns (21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_original = pd.read_csv(CSV_FILE_PATH)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated CSV with good direction predictions saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "BASE_FOLDER_NAME = 'logs'\n",
    "CSV_FILE_PATH = 'conce_bbox.csv_alternative.csv'\n",
    "CSV_FILE_PATH = os.path.join(BASE_FOLDER_NAME, CSV_FILE_PATH)\n",
    "\n",
    "# Load the original data\n",
    "df_original = pd.read_csv(CSV_FILE_PATH)\n",
    "\n",
    "# Create a copy for processing\n",
    "df = df_original.copy()\n",
    "\n",
    "# Encode labels: 1 for 'IN' or 'OUT' (good image), 0 for 'BAD' (bad image), keep NaN for now\n",
    "df['label_encoded'] = df['label_direction'].apply(lambda x: 1 if x in ['IN', 'OUT'] else 0 if x == 'BAD' else np.nan)\n",
    "\n",
    "# Prepare data for model training (exclude rows with NaN in 'label_encoded')\n",
    "df_train = df.dropna(subset=['label_encoded'])\n",
    "\n",
    "# Define features (excluding non-numeric columns and the 'label_direction', 'label_encoded' columns)\n",
    "features = [col for col in df_train.columns if col not in ['id', 'label_direction', 'label_encoded'] and df_train[col].dtype in [np.int64, np.float64]]\n",
    "target = 'label_encoded'\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train[features], df_train[target], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Gradient Boosting Classifier\n",
    "model = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the entire original dataset (make sure to handle NaNs in features if they exist)\n",
    "df_original['label_good_dir'] = model.predict(df_original[features].fillna(0))  # Using fillna(0) as an example handling method\n",
    "df_original['label_good_dir_conf'] = model.predict_proba(df_original[features].fillna(0))[:, 1]  # Confidence of being a good image\n",
    "\n",
    "# Save the updated dataframe with predictions for the entire dataset to a new CSV file\n",
    "NEW_CSV_PATH = os.path.join(BASE_FOLDER_NAME, 'updated_conce_bbox_with_good_dir_predictions.csv')\n",
    "df_original.to_csv(NEW_CSV_PATH, index=False)\n",
    "\n",
    "print(\"Updated CSV with good direction predictions saved successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
