{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add images to CSV BBOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "def find_image_name(row, image_dict):\n",
    "    \"\"\"\n",
    "    Construct the image name based on row's id and frame_number,\n",
    "    then check if it exists in the image_dict.\n",
    "    \"\"\"\n",
    "    id_frame_pattern = f\"img_{int(row['id'])}_{int(row['frame_number'])}\"\n",
    "    matched_images = [img for img in image_dict.get(int(row['id']), []) if id_frame_pattern in img]\n",
    "    return matched_images[0] if matched_images else None\n",
    "\n",
    "def append_image_names(csv_path, base_path):\n",
    "    # Load the CSV into a DataFrame\n",
    "    df = pd.read_csv(csv_path, dtype={'id': 'int64','frame_number': 'int64'})\n",
    "\n",
    "    # Dictionary to hold image names for each id\n",
    "    image_dict = {}\n",
    "\n",
    "    # List directories in the base path and filter by those matching the ids in the DataFrame\n",
    "    for dir_name in os.listdir(base_path):\n",
    "        dir_path = os.path.join(base_path, dir_name)\n",
    "        if os.path.isdir(dir_path) and dir_name.isdigit():\n",
    "            id = int(dir_name)\n",
    "            # List all images for the current id\n",
    "            image_dict[id] = [os.path.basename(x) for x in glob.glob(os.path.join(dir_path, \"*.png\"))]\n",
    "\n",
    "    # Apply the function to find the matching image name for each row\n",
    "    df['img_name'] = df.apply(lambda row: find_image_name(row, image_dict), axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "BASE_IMAGEES_PATH = '/home/diego/Documents/yolov7-tracker/imgs_conce'\n",
    "BASE_FOLDER_NAME = 'logs'\n",
    "CSV_FILE_PATH = 'conce_bbox.csv'\n",
    "\n",
    "CSV_FILE_PATH = os.path.join(BASE_FOLDER_NAME, CSV_FILE_PATH)\n",
    "updated_df = append_image_names(CSV_FILE_PATH, BASE_IMAGEES_PATH)\n",
    "\n",
    "updated_df.to_csv(CSV_FILE_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add kfold to images and add label_img, label_direction column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-importing necessary libraries and redefining the function with corrections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "# Re-defining the set_folds function\n",
    "def set_folds(csv_path, k_folds, n_images):\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Ensure 'img_name' column exists\n",
    "    if 'img_name' not in df.columns:\n",
    "        raise ValueError(\"img_name column doesn't exist in the dataset.\")\n",
    "\n",
    "    # Initialize k_fold column in original df\n",
    "    df['k_fold'] = np.nan\n",
    "    df['label_img'] = np.nan\n",
    "    df['label_direction'] = np.nan\n",
    "\n",
    "    # Filter rows where 'img_name' is not empty\n",
    "    df_filtered = df[df['img_name'] != ''].copy()\n",
    "\n",
    "    # Sort by 'id' and 'frame_number'\n",
    "    df_filtered.sort_values(by=['id', 'frame_number'], inplace=True)\n",
    "\n",
    "    # Process each ID separately in filtered df\n",
    "    for id_value in df_filtered['id'].unique():\n",
    "        subset = df_filtered[(df_filtered['id'] == id_value) & (df_filtered['img_name'].notna())]\n",
    "\n",
    "        # Apply KFold or assign all to the same fold if condition is met\n",
    "        if len(subset) < k_folds * n_images:\n",
    "            df.loc[subset.index, 'k_fold'] = 0  # Assign all to fold 0 if condition is met\n",
    "        else:\n",
    "            # Apply KFold\n",
    "            kf = KFold(n_splits=k_folds)\n",
    "            for fold, (_, test_index) in enumerate(kf.split(subset)):\n",
    "                # Select n_images per fold if specified\n",
    "                #selected_indices = test_index[:n_images] if n_images < len(test_index) else test_index Selecciona los primeros n_images\n",
    "                selected_indices = np.random.choice(test_index, min(n_images, len(test_index)), replace=False)\n",
    "                df.loc[subset.iloc[selected_indices].index, 'k_fold'] = fold\n",
    "                df.loc[subset.iloc[selected_indices].index, 'label_img'] = 0\n",
    "    return df\n",
    "\n",
    "BASE_FOLDER_NAME = 'logs'\n",
    "CSV_FILE_PATH = 'conce_bbox.csv'\n",
    "CSV_FILE_PATH = os.path.join(BASE_FOLDER_NAME, CSV_FILE_PATH)\n",
    "df_with_folds = set_folds(CSV_FILE_PATH, k_folds=5, n_images=3)\n",
    "\n",
    "df_with_folds.to_csv(CSV_FILE_PATH, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV to SQL LITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "\n",
    "def convert_csv_to_sqlite(csv_file_path, db_file_path, table_name='bbox_data'):\n",
    "    # Load the CSV file into a pandas DataFrame\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    \n",
    "    # Create a connection to the SQLite database\n",
    "    conn = sqlite3.connect(db_file_path)\n",
    "    \n",
    "    # Write the data to a SQLite table\n",
    "    df.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "    \n",
    "    # Close the connection\n",
    "    conn.close()\n",
    "\n",
    "BASE_FOLDER_NAME = 'logs'\n",
    "CSV_FILE_PATH = 'conce_bbox.csv'\n",
    "\n",
    "CSV_FILE_PATH = os.path.join(BASE_FOLDER_NAME, CSV_FILE_PATH)\n",
    "\n",
    "db_file_path = f'{BASE_FOLDER_NAME}/bbox_data.db'\n",
    "convert_csv_to_sqlite(CSV_FILE_PATH, db_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9722222222222222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8094/1637288565.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  predict_df['model_label'] = predicted_labels\n",
      "/tmp/ipykernel_8094/1637288565.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  predict_df['model_confidence'] = predicted_confidences\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated CSV saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "BASE_FOLDER_NAME = 'logs'\n",
    "CSV_FILE_PATH = 'updated_conce_bbox_area.csv'\n",
    "CSV_FILE_PATH = os.path.join(BASE_FOLDER_NAME, CSV_FILE_PATH)\n",
    "\n",
    "NEW_CSV = os.path.join(BASE_FOLDER_NAME, 'updated_conce_bbox_area_model.csv')\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv(CSV_FILE_PATH)\n",
    "\n",
    "# Preprocess your data\n",
    "# Convert '-' labels to None for easier handling\n",
    "df['label'] = df['label'].apply(lambda x: None if x == '-' else x).astype(float)\n",
    "\n",
    "# Separate the dataset into training and prediction sets\n",
    "train_df = df.dropna(subset=['label'])\n",
    "predict_df = df[df['label'].isna()]\n",
    "\n",
    "# Define features and target\n",
    "features = ['area', 'centroid_x', 'centroid_y', 'frame_number', 'overlap', 'distance_to_center', 'conf_score']\n",
    "target = 'label'\n",
    "\n",
    "# Splitting the training data for validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_df[features], train_df[target], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Gradient Boosting Classifier\n",
    "model = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Validate the model\n",
    "val_predictions = model.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val, val_predictions)\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "# Predicting on the dataset without labels\n",
    "predict_features = predict_df[features]\n",
    "predicted_labels = model.predict(predict_features)\n",
    "predicted_confidences = model.predict_proba(predict_features).max(axis=1)\n",
    "\n",
    "# Adding predictions back to the dataframe\n",
    "predict_df['model_label'] = predicted_labels\n",
    "predict_df['model_confidence'] = predicted_confidences\n",
    "\n",
    "# Combine the prediction and training dataframes\n",
    "final_df = pd.concat([train_df, predict_df], sort=False)\n",
    "\n",
    "# Save the updated dataframe to a new CSV file\n",
    "final_df.to_csv(NEW_CSV, index=False)\n",
    "\n",
    "print(\"Updated CSV saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST Show model results predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Read the CSV file\n",
    "BASE_FOLDER_NAME = 'logs'\n",
    "CSV_FILE_PATH = 'updated_conce_bbox_area.csv'\n",
    "NEW_CSV = os.path.join(BASE_FOLDER_NAME, 'updated_conce_bbox_area_model.csv')\n",
    "\n",
    "def find_matching_file_path(directory, filename_start):\n",
    "    \"\"\"\n",
    "    Searches for files in the specified directory that start with the given filename start string.\n",
    "    \n",
    "    :param directory: The directory to search within.\n",
    "    :param filename_start: The initial part of the file name to match.\n",
    "    :return: The full path to the first matching file, or None if no match is found.\n",
    "    \"\"\"\n",
    "    # Construct the search pattern\n",
    "    search_pattern = os.path.join(directory, filename_start + \"*.png\")\n",
    "    \n",
    "    # Use glob to find all files matching the pattern\n",
    "    matching_files = glob.glob(search_pattern)\n",
    "    \n",
    "    # Return the first matching file path, if any\n",
    "    if matching_files:\n",
    "        return matching_files[0]  # Return full path of the first match\n",
    "    else:\n",
    "        return ''  # No match found\n",
    "\n",
    "df = pd.read_csv(NEW_CSV)\n",
    "\n",
    "# Filter rows where `model_label` and `model_confidence` are not null\n",
    "df_filtered = df.dropna(subset=['model_label', 'model_confidence'])\n",
    "\n",
    "# Filtering rows where frame_number % 3 == 0\n",
    "#df_filtered = df_filtered[df_filtered['frame_number'] % 3 == 0]\n",
    "\n",
    "# Base path for the images\n",
    "base_path = \"/home/diego/Documents/yolov7-tracker/imgs_conce\"\n",
    "\n",
    "# Function to construct the file path\n",
    "def construct_file_path(row):\n",
    "    return os.path.join(base_path, str(int(row['id'])))\n",
    "\n",
    "# Apply the function to construct file paths\n",
    "df_filtered['file_path'] = df_filtered.apply(construct_file_path, axis=1)\n",
    "\n",
    "# Randomly select 50 images if available, or take the whole dataset if less than 50\n",
    "sample_size = min(20, len(df_filtered))\n",
    "sampled_df = df_filtered.sample(n=sample_size)\n",
    "\n",
    "rows = (sample_size + 4) // 5  # Calculate rows needed for the sample size, adjust the denominator to change columns\n",
    "cols = 5 if sample_size > 5 else sample_size  # Adjust columns based on sample size\n",
    "\n",
    "fig, axs = plt.subplots(rows, cols, figsize=(20, 8 * rows))  # Adjust figsize dynamically\n",
    "axs = axs.flatten()  # Flatten to easily loop over if it's a grid\n",
    "\n",
    "for i in range(len(axs)):\n",
    "    if i < sample_size:\n",
    "        row = sampled_df.iloc[i]\n",
    "        img_path = find_matching_file_path(row.file_path, f\"img_{row.id}_{row.frame_number}\")\n",
    "        if os.path.exists(img_path):\n",
    "            img = Image.open(img_path)\n",
    "            axs[i].imshow(img)\n",
    "            axs[i].set_title(f\"ID: {row.id}\\nFrame: {row.frame_number}\\nLabel: {int(row.model_label)}\\nConfidence: {row.model_confidence:.2f}\", fontsize=10)\n",
    "            axs[i].axis('off')\n",
    "        else:\n",
    "            axs[i].set_visible(False)\n",
    "    else:\n",
    "        axs[i].set_visible(False)  # Hide unused subplots\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data process IN/OUT/BAD Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated CSV saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "BASE_FOLDER_NAME = 'logs'\n",
    "CSV_FILE_PATH = 'conce_bbox.csv_alternative.csv'\n",
    "CSV_FILE_PATH = os.path.join(BASE_FOLDER_NAME, CSV_FILE_PATH)\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv(CSV_FILE_PATH)\n",
    "\n",
    "# Ensure the DataFrame is sorted by 'id' and 'frame_number' for correct diff calculations\n",
    "df.sort_values(by=['id', 'frame_number'], inplace=True)\n",
    "\n",
    "# Calculate Movement Features (Δx and Δy)\n",
    "df['delta_x'] = df.groupby('id')['centroid_x'].diff().fillna(0)\n",
    "df['delta_y'] = df.groupby('id')['centroid_y'].diff().fillna(0)\n",
    "\n",
    "# Calculate Aggregated Features for each ID\n",
    "aggregations = {\n",
    "    'delta_x': ['mean', 'max', 'min', 'std'],\n",
    "    'delta_y': ['mean', 'max', 'min', 'std']\n",
    "}\n",
    "aggregated_features = df.groupby('id').agg(aggregations).reset_index()\n",
    "\n",
    "# Correct the naming of the aggregated columns\n",
    "aggregated_features.columns = ['id'] + [f'{var}_{stat}' for var, stats in aggregations.items() for stat in stats]\n",
    "\n",
    "# Correctly merge aggregated features back to the original dataframe\n",
    "df = pd.merge(df, aggregated_features, on='id', how='left')\n",
    "\n",
    "# Calculate Sequence Features (net movement direction)\n",
    "df['net_movement_x'] = df.groupby('id')['delta_x'].transform('sum')\n",
    "df['net_movement_y'] = df.groupby('id')['delta_y'].transform('sum')\n",
    "\n",
    "df.to_csv(CSV_FILE_PATH, index=False)\n",
    "\n",
    "print(\"Updated CSV saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In vs Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12258/3192550527.py:15: DtypeWarning: Columns (21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(CSV_FILE_PATH)\n",
      "/tmp/ipykernel_12258/3192550527.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['label_direction_encoded'] = label_encoder.fit_transform(df_filtered['label_direction'])  # 'IN' -> 1, 'OUT' -> 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 1.0\n",
      "Updated CSV with predictions saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "BASE_FOLDER_NAME = 'logs'\n",
    "CSV_FILE_PATH = 'conce_bbox.csv_alternative.csv'\n",
    "NEW_FILE = 'updated_conce_bbox_with_predictions.csv'\n",
    "CSV_FILE_PATH = os.path.join(BASE_FOLDER_NAME, CSV_FILE_PATH)\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(CSV_FILE_PATH)\n",
    "\n",
    "# Filter out rows with 'BAD' or empty in 'label_direction'\n",
    "df_filtered = df[(df['label_direction'] == 'IN') | (df['label_direction'] == 'OUT')]\n",
    "\n",
    "# Correctly encode 'IN' as 1 and 'OUT' as 0\n",
    "label_encoder = LabelEncoder()\n",
    "df_filtered['label_direction_encoded'] = label_encoder.fit_transform(df_filtered['label_direction'])  # 'IN' -> 1, 'OUT' -> 0\n",
    "\n",
    "# Define features (make sure to only include numeric columns and exclude any text columns)\n",
    "features = [col for col in df_filtered.columns if col not in ['id', 'label_direction', 'label_direction_encoded'] and df_filtered[col].dtype in [np.int64, np.float64]]\n",
    "target = 'label_direction_encoded'\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_filtered[features], df_filtered[target], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Gradient Boosting Classifier\n",
    "model = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "predictions = model.predict(X_test)\n",
    "prediction_probs = model.predict_proba(X_test)[:, 1]  # Probability of being 'IN'\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "# Prepare the entire dataset for prediction\n",
    "# First, ensure only numeric features are used\n",
    "df['label_direction_p'] = np.nan  # Initialize column for model predictions\n",
    "df['label_direction_p_conf'] = np.nan  # Initialize column for prediction confidence\n",
    "\n",
    "# Predicting on rows needing prediction (assuming 'BAD' or not labeled)\n",
    "predict_features = df[features]\n",
    "df['label_direction_p'] = model.predict(predict_features)\n",
    "df['label_direction_p_conf'] = model.predict_proba(predict_features)[:, 1]  # Probability of being 'IN'\n",
    "\n",
    "# Mapping numeric predictions back to 'IN' or 'OUT'\n",
    "df['label_direction_p'] = label_encoder.inverse_transform(df['label_direction_p'].astype(int))\n",
    "\n",
    "# Save the updated dataframe to a new CSV file\n",
    "NEW_CSV_PATH = os.path.join(BASE_FOLDER_NAME, NEW_FILE)\n",
    "df.to_csv(NEW_CSV_PATH, index=False)\n",
    "\n",
    "print(\"Updated CSV with predictions saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In|OUT vs BAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12258/3214391915.py:13: DtypeWarning: Columns (21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_original = pd.read_csv(CSV_FILE_PATH)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated CSV with good direction predictions saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "BASE_FOLDER_NAME = 'logs'\n",
    "CSV_FILE_PATH = 'conce_bbox.csv_alternative.csv'\n",
    "CSV_FILE_PATH = os.path.join(BASE_FOLDER_NAME, CSV_FILE_PATH)\n",
    "\n",
    "# Load the original data\n",
    "df_original = pd.read_csv(CSV_FILE_PATH)\n",
    "\n",
    "# Create a copy for processing\n",
    "df = df_original.copy()\n",
    "\n",
    "# Encode labels: 1 for 'IN' or 'OUT' (good image), 0 for 'BAD' (bad image), keep NaN for now\n",
    "df['label_encoded'] = df['label_direction'].apply(lambda x: 1 if x in ['IN', 'OUT'] else 0 if x == 'BAD' else np.nan)\n",
    "\n",
    "# Prepare data for model training (exclude rows with NaN in 'label_encoded')\n",
    "df_train = df.dropna(subset=['label_encoded'])\n",
    "\n",
    "# Define features (excluding non-numeric columns and the 'label_direction', 'label_encoded' columns)\n",
    "features = [col for col in df_train.columns if col not in ['id', 'label_direction', 'label_encoded'] and df_train[col].dtype in [np.int64, np.float64]]\n",
    "target = 'label_encoded'\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train[features], df_train[target], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Gradient Boosting Classifier\n",
    "model = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the entire original dataset (make sure to handle NaNs in features if they exist)\n",
    "df_original['label_good_dir'] = model.predict(df_original[features].fillna(0))  # Using fillna(0) as an example handling method\n",
    "df_original['label_good_dir_conf'] = model.predict_proba(df_original[features].fillna(0))[:, 1]  # Confidence of being a good image\n",
    "\n",
    "# Save the updated dataframe with predictions for the entire dataset to a new CSV file\n",
    "NEW_CSV_PATH = os.path.join(BASE_FOLDER_NAME, 'updated_conce_bbox_with_good_dir_predictions.csv')\n",
    "df_original.to_csv(NEW_CSV_PATH, index=False)\n",
    "\n",
    "print(\"Updated CSV with good direction predictions saved successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
