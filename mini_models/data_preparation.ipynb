{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "BASE_IMAGES_PATH = '/home/diego/Documents/yolov7-tracker/imgs_conce'\n",
    "BASE_FOLDER_NAME = 'results'\n",
    "FILE_NAME = 'conce_bbox.csv'\n",
    "CSV_FILE_PATH = os.path.join(BASE_FOLDER_NAME,FILE_NAME)\n",
    "TMP_120 = os.path.join(BASE_FOLDER_NAME,'data_prep_conce_bbox.csv')\n",
    "TMP_130 = os.path.join(BASE_FOLDER_NAME,f\"130_{FILE_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add images to CSV BBOX [110] [OBSOLETA BORRAR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# import glob\n",
    "\n",
    "# def find_image_name(row, image_dict):\n",
    "#     \"\"\"\n",
    "#     Construct the image name based on row's id and frame_number,\n",
    "#     then check if it exists in the image_dict.\n",
    "#     \"\"\"\n",
    "#     id_frame_pattern = f\"img_{int(row['id'])}_{int(row['frame_number'])}\"\n",
    "#     matched_images = [img for img in image_dict.get(int(row['id']), []) if id_frame_pattern in img]\n",
    "#     return matched_images[0] if matched_images else None\n",
    "\n",
    "# def append_image_names(csv_path, base_path):\n",
    "#     # Load the CSV into a DataFrame\n",
    "#     df = pd.read_csv(csv_path, dtype={'id': 'int64','frame_number': 'int64'})\n",
    "\n",
    "#     # Dictionary to hold image names for each id\n",
    "#     image_dict = {}\n",
    "\n",
    "#     # List directories in the base path and filter by those matching the ids in the DataFrame\n",
    "#     for dir_name in os.listdir(base_path):\n",
    "#         dir_path = os.path.join(base_path, dir_name)\n",
    "#         if os.path.isdir(dir_path) and dir_name.isdigit():\n",
    "#             id = int(dir_name)\n",
    "#             # List all images for the current id\n",
    "#             image_dict[id] = [os.path.basename(x) for x in glob.glob(os.path.join(dir_path, \"*.png\"))]\n",
    "\n",
    "#     # Apply the function to find the matching image name for each row\n",
    "#     df['img_name'] = df.apply(lambda row: find_image_name(row, image_dict), axis=1)\n",
    "    \n",
    "#     return df\n",
    "\n",
    "\n",
    "# updated_df = append_image_names(CSV_FILE_PATH, BASE_IMAGES_PATH)\n",
    "# updated_df.to_csv(CSV_FILE_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add kfold to images and add label_img, label_direction column [110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-importing necessary libraries and redefining the function with corrections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "# Re-defining the set_folds function\n",
    "def set_folds(csv_path, k_folds, n_images):\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Ensure 'img_name' column exists\n",
    "    if 'img_name' not in df.columns:\n",
    "        raise ValueError(\"img_name column doesn't exist in the dataset.\")\n",
    "\n",
    "    # Initialize k_fold column in original df\n",
    "    df['k_fold'] = np.nan\n",
    "    df['label_img'] = np.nan\n",
    "\n",
    "    # Filter rows where 'img_name' is not empty\n",
    "    df_filtered = df[df['img_name'] != ''].copy()\n",
    "\n",
    "    # Sort by 'id' and 'frame_number'\n",
    "    df_filtered.sort_values(by=['id', 'frame_number'], inplace=True)\n",
    "\n",
    "    # Process each ID separately in filtered df\n",
    "    for id_value in df_filtered['id'].unique():\n",
    "        subset = df_filtered[(df_filtered['id'] == id_value) & (df_filtered['img_name'].notna())]\n",
    "\n",
    "        # Apply KFold or assign all to the same fold if condition is met\n",
    "        if len(subset) < k_folds * n_images:\n",
    "            df.loc[subset.index, 'k_fold'] = 0  # Assign all to fold 0 if condition is met\n",
    "        else:\n",
    "            # Apply KFold\n",
    "            kf = KFold(n_splits=k_folds)\n",
    "            for fold, (_, test_index) in enumerate(kf.split(subset)):\n",
    "                # Select n_images per fold if specified\n",
    "                #selected_indices = test_index[:n_images] if n_images < len(test_index) else test_index Selecciona los primeros n_images\n",
    "                selected_indices = np.random.choice(test_index, min(n_images, len(test_index)), replace=False)\n",
    "                df.loc[subset.iloc[selected_indices].index, 'k_fold'] = fold\n",
    "                df.loc[subset.iloc[selected_indices].index, 'label_img'] = 0\n",
    "    return df\n",
    "\n",
    "\n",
    "df_with_folds = set_folds(CSV_FILE_PATH, k_folds=5, n_images=3)\n",
    "df_with_folds.to_csv(CSV_FILE_PATH, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data process IN vs OUT Feature Engineering [FINAL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_110928/2666431410.py:48: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('id').apply(calculate_dynamic_segments_deltas, num_segments=num_segments)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated CSV saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "# Assuming df is your DataFrame loaded from the CSV file\n",
    "\n",
    "# Number of desired segments\n",
    "num_segments = 10  # Or any other number you choose\n",
    "\n",
    "# Function to calculate dynamic segments and deltas\n",
    "def calculate_dynamic_segments_deltas(group, num_segments):\n",
    "    total_rows = len(group)\n",
    "    rows_per_segment = max(1, np.ceil(total_rows / num_segments))\n",
    "    \n",
    "    # Assign segments based on dynamic rows_per_segment\n",
    "    group['segment'] = (np.arange(total_rows) // rows_per_segment) + 1\n",
    "    \n",
    "    # Calculate mean centroid for each segment\n",
    "    segment_means = group.groupby('segment')[['centroid_x', 'centroid_y']].mean().reset_index()\n",
    "    \n",
    "    # Calculate deltas between segment means\n",
    "    segment_means['delta_mean_x'] = segment_means['centroid_x'].diff().fillna(0)\n",
    "    segment_means['delta_mean_y'] = segment_means['centroid_y'].diff().fillna(0)\n",
    "    \n",
    "    # Merge the delta values back to the original group based on segment\n",
    "    group = pd.merge(group, segment_means[['segment', 'delta_mean_x', 'delta_mean_y']], on='segment', how='left')\n",
    "    \n",
    "    # Optionally drop the 'segment' column if it's no longer needed\n",
    "    group.drop('segment', axis=1, inplace=True)\n",
    "    \n",
    "    return group\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv(CSV_FILE_PATH)\n",
    "\n",
    "\n",
    "## TEMPORAL ###\n",
    "label_index = df.columns.get_loc('label_direction')\n",
    "df = df.iloc[:, :label_index + 1]\n",
    "## TEMPORAL ###\n",
    "\n",
    "# Ensure the DataFrame is sorted by 'id' and 'frame_number' for correct diff calculations\n",
    "df.sort_values(by=['id', 'frame_number'], inplace=True)\n",
    "\n",
    "# Apply the function to each 'id' group with the number of desired segments\n",
    "df = df.groupby('id').apply(calculate_dynamic_segments_deltas, num_segments=num_segments)\n",
    "# Reset index after groupby operation\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Calculate Movement Features (Δx and Δy)\n",
    "df['delta_x'] = df.groupby('id')['centroid_x'].diff().fillna(0)\n",
    "df['delta_y'] = df.groupby('id')['centroid_y'].diff().fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calculate Initial and Final Position Features without using frame_number\n",
    "df['initial_x'] = df.groupby('id')['centroid_x'].transform('first')\n",
    "df['initial_y'] = df.groupby('id')['centroid_y'].transform('first')\n",
    "df['final_x'] = df.groupby('id')['centroid_x'].transform('last')\n",
    "df['final_y'] = df.groupby('id')['centroid_y'].transform('last')\n",
    "\n",
    "# Calculate total distance traveled without frame_number\n",
    "# df['total_distance'] = np.sqrt((df['final_x'] - df['initial_x'])**2 + (df['final_y'] - df['initial_y'])**2)\n",
    "\n",
    "# Assuming that the ordering of frames is implicitly represented by their position in the DataFrame,\n",
    "# you can calculate movement features based on the assumption that earlier rows are earlier in time\n",
    "# However, since we're not using frame_number for temporal division, we'll skip avg_move_per_frame_x and avg_move_per_frame_y\n",
    "\n",
    "# Instead, focus on proportional change without direct temporal context\n",
    "# Calculate the difference in positions as a proxy for movement direction and magnitude\n",
    "df['delta_x_final'] = df['final_x'] - df['initial_x']\n",
    "df['delta_y_final'] = df['final_y'] - df['initial_y']\n",
    "\n",
    "# Optionally, calculate normalized movement direction vectors if needed\n",
    "# df['direction_x'] = df['delta_x'] / df['total_distance']\n",
    "# df['direction_y'] = df['delta_y'] / df['total_distance']\n",
    "# df['direction_x'] = df['direction_x'].fillna(0)  # Handle division by zero if total_distance is 0\n",
    "# df['direction_y'] = df['direction_y'].fillna(0)\n",
    "\n",
    "# Calculate Aggregated Features for each ID\n",
    "# aggregations = {\n",
    "#     'delta_x': ['mean','max', 'min','std'],\n",
    "#     'delta_y': ['mean','max', 'min','std'],\n",
    "# }\n",
    "aggregations = {\n",
    "    'centroid_x': ['std'],\n",
    "    'centroid_y': ['std'],\n",
    "}\n",
    "aggregated_features = df.groupby('id').agg(aggregations).reset_index()\n",
    "aggregated_features.columns = ['id'] + [f'{var}_{stat}' for var, stats in aggregations.items() for stat in stats]\n",
    "\n",
    "# Correctly merge aggregated features back to the original dataframe\n",
    "df = pd.merge(df, aggregated_features, on='id', how='left')\n",
    "\n",
    "# Calculate Sequence Features (net movement direction)\n",
    "df['net_movement_x'] = df.groupby('id')['delta_x'].transform('sum')\n",
    "df['net_movement_y'] = df.groupby('id')['delta_y'].transform('sum')\n",
    "\n",
    "df['net_movement_x_mean'] = df.groupby('id')['delta_mean_x'].transform('sum')\n",
    "df['net_movement_y_mean'] = df.groupby('id')['delta_mean_y'].transform('sum')\n",
    "\n",
    "df.to_csv(TMP_120, index=False)\n",
    "\n",
    "print(\"Updated CSV saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Process IN_OUT vs BAD Feature Engineering [130]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated CSV saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv(CSV_FILE_PATH)\n",
    "\n",
    "\n",
    "## TEMPORAL ###\n",
    "label_index = df.columns.get_loc('label_direction')\n",
    "df = df.iloc[:, :label_index + 1]\n",
    "## TEMPORAL ###\n",
    "\n",
    "# Ensure the DataFrame is sorted by 'id' and 'frame_number' for correct diff calculations\n",
    "df.sort_values(by=['id', 'frame_number'], inplace=True)\n",
    "\n",
    "# Calculate Movement Features (Δx and Δy)\n",
    "df['delta_x'] = df.groupby('id')['centroid_x'].diff().fillna(0)\n",
    "df['delta_y'] = df.groupby('id')['centroid_y'].diff().fillna(0)\n",
    "\n",
    "# Calculate Aggregated Features for each ID\n",
    "aggregations = {\n",
    "    'delta_x': ['mean','max', 'min','std'],\n",
    "    'delta_y': ['mean','max', 'min','std'],\n",
    "}\n",
    "aggregated_features = df.groupby('id').agg(aggregations).reset_index()\n",
    "\n",
    "# Correct the naming of the aggregated columns\n",
    "aggregated_features.columns = ['id'] + [f'{var}_{stat}' for var, stats in aggregations.items() for stat in stats]\n",
    "\n",
    "# Correctly merge aggregated features back to the original dataframe\n",
    "df = pd.merge(df, aggregated_features, on='id', how='left')\n",
    "\n",
    "# Calculate Sequence Features (net movement direction)\n",
    "df['net_movement_x'] = df.groupby('id')['delta_x'].transform('sum')\n",
    "df['net_movement_y'] = df.groupby('id')['delta_y'].transform('sum')\n",
    "\n",
    "df.to_csv(TMP_130, index=False)\n",
    "\n",
    "print(\"Updated CSV saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV to SQL LITE [110] OJO CON CORRERLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import sqlite3\n",
    "# import os\n",
    "\n",
    "# def convert_csv_to_sqlite(csv_file_path, db_file_path, table_name='bbox_data'):\n",
    "#     # Load the CSV file into a pandas DataFrame\n",
    "#     df = pd.read_csv(csv_file_path)\n",
    "    \n",
    "#     # Create a connection to the SQLite database\n",
    "#     conn = sqlite3.connect(db_file_path)\n",
    "    \n",
    "#     # Write the data to a SQLite table\n",
    "#     df.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "    \n",
    "#     # Close the connection\n",
    "#     conn.close()\n",
    "\n",
    "# BASE_FOLDER_NAME = 'results'\n",
    "# CSV_FILE = 'santos_dumont_bbox.csv'\n",
    "\n",
    "# CSV_FILE_PATH = os.path.join(BASE_FOLDER_NAME, CSV_FILE)\n",
    "\n",
    "# db_file_path = f'{BASE_FOLDER_NAME}/{CSV_FILE.replace(\".csv\", \".db\")}'\n",
    "# convert_csv_to_sqlite(CSV_FILE_PATH, db_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQLite -> CSV [120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "\n",
    "def convert_sqlite_to_csv(db_file_path, csv_file_path, table_name='bbox_data'):\n",
    "    # Create a connection to the SQLite database\n",
    "    conn = sqlite3.connect(db_file_path)\n",
    "    \n",
    "    # Read the table into a pandas DataFrame\n",
    "    df = pd.read_sql_query(f\"SELECT * FROM {table_name}\", conn)\n",
    "    \n",
    "    # Write the DataFrame to a CSV file\n",
    "    df.to_csv(csv_file_path, index=False)\n",
    "    \n",
    "    # Close the connection\n",
    "    conn.close()\n",
    "\n",
    "BASE_FOLDER_NAME = 'results'\n",
    "\n",
    "db_file_path = f'{BASE_FOLDER_NAME}/bbox_data.db'\n",
    "CSV_FILE_PATH = 'from_sql_bbox.csv'\n",
    "CSV_FILE_PATH = os.path.join(BASE_FOLDER_NAME, CSV_FILE_PATH)\n",
    "\n",
    "convert_sqlite_to_csv(db_file_path, CSV_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updates labels from SQLite to a new bbox.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9095/3189779983.py:22: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'BAD' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  conce_bbox_df.at[index, 'label_direction'] = label_direction_map[ID]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files into pandas dataframes\n",
    "conce_bbox_df = pd.read_csv('results/conce_bbox.csv')\n",
    "from_sql_bbox_df = pd.read_csv('results/from_sql_bbox.csv')\n",
    "\n",
    "# Create mappings for updates\n",
    "label_img_map = from_sql_bbox_df.set_index('img_name')['label_img'].to_dict()\n",
    "label_direction_map = from_sql_bbox_df.set_index('id')['label_direction'].to_dict()\n",
    "\n",
    "# Update conce_bbox DataFrame\n",
    "for index, row in conce_bbox_df.iterrows():\n",
    "    img_name = row['img_name']\n",
    "    ID = row['id']\n",
    "    \n",
    "    # Check and update label_img if img_name matches\n",
    "    if img_name in label_img_map:\n",
    "        conce_bbox_df.at[index, 'label_img'] = label_img_map[img_name]\n",
    "    \n",
    "    # Check and update label_direction if ID matches\n",
    "    if ID in label_direction_map:\n",
    "        conce_bbox_df.at[index, 'label_direction'] = label_direction_map[ID]\n",
    "\n",
    "# Optionally, save the updated dataframe back to a CSV\n",
    "conce_bbox_df.to_csv('results/conce_bbox.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
