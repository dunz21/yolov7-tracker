{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Image Selection Pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib  # For saving and loading the model\n",
    "\n",
    "def train_and_save_model(training_data_path, model_save_path):\n",
    "    INTEREST_LABEL = 'label_img'\n",
    "    interest_values = [1, 2]\n",
    "\n",
    "    only_train_data = pd.read_csv(training_data_path)\n",
    "    only_train_data = only_train_data.dropna(subset=['img_name'])\n",
    "    only_train_data = only_train_data[only_train_data[INTEREST_LABEL].isin(interest_values)]\n",
    "\n",
    "    features = ['area', 'centroid_x', 'centroid_y', 'frame_number', 'overlap', 'distance_to_center', 'conf_score']\n",
    "    target = INTEREST_LABEL\n",
    "\n",
    "    assert all(f in only_train_data.columns for f in features + [target]), \"Some required columns are missing.\"\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(only_train_data[features], only_train_data[target], test_size=0.2, random_state=42)\n",
    "\n",
    "    model = GradientBoostingClassifier(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Save the trained model\n",
    "    joblib.dump(model, model_save_path)\n",
    "\n",
    "    val_predictions = model.predict(X_val)\n",
    "    val_accuracy = accuracy_score(y_val, val_predictions)\n",
    "    print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "def predict_and_save_results(model_weights_path, csv_file_path):\n",
    "    features = ['area', 'centroid_x', 'centroid_y', 'frame_number', 'overlap', 'distance_to_center', 'conf_score']\n",
    "    \n",
    "    # Load the model\n",
    "    model = joblib.load(model_weights_path)\n",
    "    \n",
    "    # Load the CSV file for prediction\n",
    "    data = pd.read_csv(csv_file_path)\n",
    "    assert all(f in data.columns for f in features), \"Some required columns for prediction are missing.\"\n",
    "    \n",
    "    predictions = model.predict(data[features])\n",
    "    predicted_confidences = model.predict_proba(data[features]).max(axis=1)\n",
    "    \n",
    "    # Add predictions to the dataframe\n",
    "    data['model_label_img'] = predictions\n",
    "    data['model_label_conf'] = predicted_confidences\n",
    "    \n",
    "    data['model_label_conf'] = data['model_label_conf'].round(2)\n",
    "    \n",
    "    # Save the modified dataframe to a new CSV file\n",
    "    predicted_csv_path = os.path.splitext(csv_file_path)[0] + \"_img_selection_predicted.csv\"\n",
    "    data.to_csv(predicted_csv_path, index=False)\n",
    "    print(f\"Predictions saved to: {predicted_csv_path}\")\n",
    "    \n",
    "\n",
    "# train_and_save_model('/home/diego/Documents/yolov7-tracker/mini_models/results/from_sql_bbox.csv', '/home/diego/Documents/yolov7-tracker/mini_models/results/image_selection_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to: /home/diego/Documents/yolov7-tracker/logs/santos_dumont_bbox_img_selection_predicted.csv\n"
     ]
    }
   ],
   "source": [
    "predict_and_save_results('/home/diego/Documents/yolov7-tracker/mini_models/results/image_selection_model.pkl','/home/diego/Documents/yolov7-tracker/logs/santos_dumont_bbox.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Image Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "BASE_FOLDER_NAME = 'results'\n",
    "CSV_FILE_NAME = 'conce_bbox.csv'\n",
    "CSV_FILE_PATH = os.path.join(BASE_FOLDER_NAME, CSV_FILE_NAME)\n",
    "\n",
    "\n",
    "MODEL_RESULT = os.path.join(BASE_FOLDER_NAME, f'model_img_selction{CSV_FILE_NAME}')\n",
    "\n",
    "\n",
    "\n",
    "INTEREST_LABEL = 'label_img'\n",
    "interest_values = [1, 2]  # Define the values you're interested in BAD and GOOD images\n",
    "\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv(CSV_FILE_PATH)\n",
    "\n",
    "# Preprocess your data\n",
    "df[INTEREST_LABEL] = df[INTEREST_LABEL].apply(lambda x: x if x in interest_values else None).astype(float)\n",
    "\n",
    "#### TWEAK DIEGO ####\n",
    "# XQ la data la tengo en otro CSV que es de entrenamitento\n",
    "CSV_FILE_PATH = os.path.join(BASE_FOLDER_NAME, 'from_sql_bbox.csv')\n",
    "only_train_data = pd.read_csv(CSV_FILE_PATH)\n",
    "only_train_data = only_train_data.dropna(subset=['img_name'])\n",
    "only_train_data = only_train_data[only_train_data[INTEREST_LABEL].isin(interest_values)]\n",
    "#### TWEAK DIEGO ####\n",
    "\n",
    "\n",
    "\n",
    "# Further filter the DataFrame to only include rows with ID lower than 1300 for training\n",
    "train_df = df[(df['id'] < 1300) & df[INTEREST_LABEL].notna()]\n",
    "\n",
    "# For prediction, you'd consider rows beyond ID 1300 or those not fitting the interest labels\n",
    "predict_df = df[(df['id'] >= 1300) | df[INTEREST_LABEL].isna()] ##### OJO ESTO\n",
    "\n",
    "#Filter only the rows that has img_name\n",
    "predict_df = predict_df.dropna(subset=['img_name'])\n",
    "\n",
    "# Define features and target\n",
    "features = ['area', 'centroid_x', 'centroid_y', 'frame_number', 'overlap', 'distance_to_center', 'conf_score']\n",
    "target = INTEREST_LABEL\n",
    "\n",
    "# Splitting the training data for validation ### TWEEK DIEGO ###\n",
    "X_train, X_val, y_train, y_val = train_test_split(only_train_data[features], only_train_data[target], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Gradient Boosting Classifier\n",
    "model = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Validate the model\n",
    "val_predictions = model.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val, val_predictions)\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "# Predicting on the dataset without labels\n",
    "predict_features = predict_df[features]\n",
    "predicted_labels = model.predict(predict_features)\n",
    "predicted_confidences = model.predict_proba(predict_features).max(axis=1)\n",
    "\n",
    "# Adding predictions back to the dataframe\n",
    "predict_df['model_label_img'] = predicted_labels\n",
    "predict_df['model_label_conf'] = predicted_confidences\n",
    "\n",
    "# Combine the prediction and training dataframes\n",
    "final_df = pd.concat([train_df, predict_df], sort=False)\n",
    "\n",
    "# Save the updated dataframe to a new CSV file\n",
    "# final_df.to_csv(MODEL_RESULT, index=False) #-> SAVE\n",
    "\n",
    "\n",
    "#### PREDICT IN ALL DATA ####\n",
    "df_total = df.dropna(subset=['img_name'])\n",
    "predict_features = df_total[features]\n",
    "predicted_labels = model.predict(predict_features)\n",
    "predicted_confidences = model.predict_proba(predict_features).max(axis=1)\n",
    "\n",
    "# Adding predictions back to the dataframe\n",
    "df_total['model_label_img'] = predicted_labels\n",
    "df_total['model_label_conf'] = predicted_confidences.round(2)\n",
    "total = os.path.join(BASE_FOLDER_NAME, f'total_model_img_selction_{CSV_FILE_NAME}')\n",
    "df_total.to_csv(total, index=False) #-> SAVE\n",
    "\n",
    "#### PREDICT IN ALL DATA ####\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Updated CSV saved successfully.\")\n",
    "### OJO DIEGO ###\n",
    "# Este resultado se le quitan todas las rows que no tienen img_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# BASE_FOLDER_NAME = 'results'\n",
    "# CSV_FILE_NAME = 'conce_bbox.csv'\n",
    "# CSV_FILE_PATH = os.path.join(BASE_FOLDER_NAME, CSV_FILE_NAME)\n",
    "\n",
    "\n",
    "# MODEL_RESULT = os.path.join(BASE_FOLDER_NAME, f'model_img_selction{CSV_FILE_NAME}')\n",
    "\n",
    "\n",
    "\n",
    "INTEREST_LABEL = 'label_img'\n",
    "interest_values = [1, 2]  # Define the values you're interested in BAD and GOOD images\n",
    "\n",
    "\n",
    "# # Load your data\n",
    "# df = pd.read_csv(CSV_FILE_PATH)\n",
    "\n",
    "# # Preprocess your data\n",
    "# df[INTEREST_LABEL] = df[INTEREST_LABEL].apply(lambda x: x if x in interest_values else None).astype(float)\n",
    "\n",
    "#### TWEAK DIEGO ####\n",
    "# XQ la data la tengo en otro CSV que es de entrenamitento\n",
    "CSV_FILE_PATH = os.path.join(BASE_FOLDER_NAME, 'from_sql_bbox.csv')\n",
    "only_train_data = pd.read_csv(CSV_FILE_PATH)\n",
    "only_train_data = only_train_data.dropna(subset=['img_name'])\n",
    "only_train_data = only_train_data[only_train_data[INTEREST_LABEL].isin(interest_values)]\n",
    "#### TWEAK DIEGO ####\n",
    "\n",
    "\n",
    "\n",
    "# Further filter the DataFrame to only include rows with ID lower than 1300 for training\n",
    "# train_df = df[(df['id'] < 1300) & df[INTEREST_LABEL].notna()]\n",
    "\n",
    "# For prediction, you'd consider rows beyond ID 1300 or those not fitting the interest labels\n",
    "# predict_df = df[(df['id'] >= 1300) | df[INTEREST_LABEL].isna()] ##### OJO ESTO\n",
    "\n",
    "#Filter only the rows that has img_name\n",
    "# predict_df = predict_df.dropna(subset=['img_name'])\n",
    "\n",
    "# Define features and target\n",
    "features = ['area', 'centroid_x', 'centroid_y', 'frame_number', 'overlap', 'distance_to_center', 'conf_score']\n",
    "target = INTEREST_LABEL\n",
    "\n",
    "# Splitting the training data for validation ### TWEEK DIEGO ###\n",
    "X_train, X_val, y_train, y_val = train_test_split(only_train_data[features], only_train_data[target], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Gradient Boosting Classifier\n",
    "model = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Validate the model\n",
    "val_predictions = model.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val, val_predictions)\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "# Predicting on the dataset without labels\n",
    "# predict_features = predict_df[features]\n",
    "# predicted_labels = model.predict(predict_features)\n",
    "# predicted_confidences = model.predict_proba(predict_features).max(axis=1)\n",
    "\n",
    "# # Adding predictions back to the dataframe\n",
    "# predict_df['model_label_img'] = predicted_labels\n",
    "# predict_df['model_label_conf'] = predicted_confidences\n",
    "\n",
    "# # Combine the prediction and training dataframes\n",
    "# final_df = pd.concat([train_df, predict_df], sort=False)\n",
    "\n",
    "# # Save the updated dataframe to a new CSV file\n",
    "# # final_df.to_csv(MODEL_RESULT, index=False) #-> SAVE\n",
    "\n",
    "\n",
    "# #### PREDICT IN ALL DATA ####\n",
    "# df_total = df.dropna(subset=['img_name'])\n",
    "# predict_features = df_total[features]\n",
    "# predicted_labels = model.predict(predict_features)\n",
    "# predicted_confidences = model.predict_proba(predict_features).max(axis=1)\n",
    "\n",
    "# # Adding predictions back to the dataframe\n",
    "# df_total['model_label_img'] = predicted_labels\n",
    "# df_total['model_label_conf'] = predicted_confidences.round(2)\n",
    "# total = os.path.join(BASE_FOLDER_NAME, f'total_model_img_selction_{CSV_FILE_NAME}')\n",
    "# df_total.to_csv(total, index=False) #-> SAVE\n",
    "\n",
    "# #### PREDICT IN ALL DATA ####\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"Updated CSV saved successfully.\")\n",
    "# ### OJO DIEGO ###\n",
    "# # Este resultado se le quitan todas las rows que no tienen img_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Image Selection Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ESTE RECIBE final_df = pd.concat([train_df, predict_df], sort=False), donde solo van a haber datos en model_label_img y model_label_conf\n",
    "# Para que no cuente el train\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from utils import calculate_confidence_distribution\n",
    "FOLDER_PATH_IMGS = '/home/diego/Documents/yolov7-tracker/imgs_conce/'\n",
    "\n",
    "\n",
    "# Filter final_df to include only rows where both 'model_label_img' and INTEREST_LABEL are not null\n",
    "comparison_df = final_df.dropna(subset=['model_label_img', INTEREST_LABEL])\n",
    "# Extract model predictions and actual labels\n",
    "y_pred = comparison_df['model_label_img']\n",
    "y_true = comparison_df[INTEREST_LABEL]\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Display the confusion matrix using ConfusionMatrixDisplay\n",
    "cmd = ConfusionMatrixDisplay(cm, display_labels=interest_values)\n",
    "cmd.plot(cmap=\"Blues\")\n",
    "cmd.ax_.set(xlabel='Predicted labels', ylabel='True labels', title='Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Generate the classification report\n",
    "report = classification_report(y_true, y_pred, target_names=[str(label) for label in interest_values])\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Assuming 'comparison_df' contains the actual (INTEREST_LABEL) and predicted ('model_label_img') labels\n",
    "# Let's identify False Negatives: actual label is in interest_values, but predicted is not.\n",
    "total_wrong_filter = (comparison_df[INTEREST_LABEL].isin(interest_values)) & (comparison_df['model_label_img'] != comparison_df[INTEREST_LABEL])\n",
    "false_negative_filter = (comparison_df[INTEREST_LABEL] == 1) & (comparison_df['model_label_img'] == 2) # False Negative filter\n",
    "false_positive_filter = (comparison_df[INTEREST_LABEL] == 2) & (comparison_df['model_label_img'] == 1) # False Positive filter\n",
    "\n",
    "false_negatives = comparison_df[false_negative_filter][:20]\n",
    "\n",
    "# Define the grid size for plotting\n",
    "n_rows = 2  # Adjust based on the number of images you want per column\n",
    "n_cols = (len(false_negatives) + 1) // n_rows\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 4, n_rows * 4))\n",
    "fig.suptitle('False Negative Images')\n",
    "\n",
    "# Flatten the axes array for easy indexing\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i,(index, row) in enumerate(false_negatives.iterrows()):\n",
    "    img_path = os.path.join(FOLDER_PATH_IMGS,row['img_name'].split('_')[1], row['img_name'])\n",
    "    img = mpimg.imread(img_path)\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].set_title(f\"ID: {row['img_name'].split('_')[1]} frame: {row['img_name'].split('_')[2]} \\n true: {row['label_img']} pred: {row['model_label_img']} conf: {row['model_label_conf']:.2f}\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "# Hide any empty subplots\n",
    "for j in range(i + 1, n_rows * n_cols):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "# Save the figure to disk\n",
    "save_path = 'logs/imageSelectionResults.png'  # Specify your desired path and filename\n",
    "plt.savefig(save_path, dpi=300)  # Adjust DPI for higher resolution images\n",
    "plt.show()\n",
    "\n",
    "calculate_confidence_distribution(comparison_df, label_direction_column='label_img', model_label_direction_column='model_label_img', model_label_direction_conf_column='model_label_conf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully converted to Excel and saved at path_to_excel_file.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to the log file\n",
    "log_file_path = '/home/diego/Documents/yolov7-tracker/mini_models/parameter_tuning_log.txt'\n",
    "\n",
    "# Initialize an empty list to store the dictionaries\n",
    "data = []\n",
    "\n",
    "# Open the log file and process each block of text\n",
    "with open(log_file_path, 'r') as file:\n",
    "    entry = {}\n",
    "    for line in file:\n",
    "        if line.startswith('K1:'):\n",
    "            # Split the line by ', ' and extract values\n",
    "            params = line.split(', ')\n",
    "            entry['K1'] = int(params[0].split(': ')[1])\n",
    "            entry['K2'] = int(params[1].split(': ')[1])\n",
    "            entry['LAMBDA'] = float(params[2].split(': ')[1])\n",
    "        elif line.startswith('Rank1:'):\n",
    "            entry['Rank1'] = float(line.split(': ')[1].replace('%', ''))\n",
    "        elif line.startswith('Rank5:'):\n",
    "            entry['Rank5'] = float(line.split(': ')[1].replace('%', ''))\n",
    "        elif line.startswith('Matches#Rank5:'):\n",
    "            entry['Matches_Rank5'] = float(line.split(': ')[1].replace('%', ''))\n",
    "        elif line.startswith('mAP:'):\n",
    "            entry['mAP'] = float(line.split(': ')[1].replace('%', ''))\n",
    "        elif line.strip() == '===============================':\n",
    "            # End of a block of data, add the dictionary to the list and reset it\n",
    "            data.append(entry)\n",
    "            entry = {}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Convert the DataFrame to an Excel file\n",
    "excel_file_path = 'path_to_excel_file.xlsx'\n",
    "df.to_excel(excel_file_path, index=False)\n",
    "\n",
    "print(f\"Data has been successfully converted to Excel and saved at {excel_file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
